{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: we are in the process of updating the links in this notebook. If a link doesn't work, please open an issue and we'll rectify it ASAP. Thanks for your understanding!\n",
        "\n",
        "Links to add:\n",
        "* Cell 1: full system for auto-meeting notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplM9PSe8djM"
      },
      "source": [
        "# Recipes for better meeting notes summaries\n",
        "\n",
        "This notebook builds on top of [our guide towards building better meeting summaries](https://github.com/cohere-ai/notebooks/blob/22dceb0ce27d73cc27f74cbf2b7c82568cbd26b7/notebooks/Meeting_Summaries_General_%26_LangChain.ipynb). In that guide, we saw how to use Command-R to summarize meeting transcripts with a focus on imparting distinct formatting requirements.\n",
        "\n",
        "In this notebook, we'll cover useful recipes that we use internally to summarize our own meeting notes automatically (see [this guide](https://colab.research.google.com/drive/1BTAAV4ss-iPtxT0ueS7djbIIwPSt3Dpp) for an outline of the full system). We will focus on extracting specific items from the meeting notes, namely:\n",
        "\n",
        "1. Extract action items with assignees (who's on the hook for which task)\n",
        "2. Summarise speaker perspectives (who said what)\n",
        "3. Focus on a narrow topic (what was said about a given topic)\n",
        "\n",
        "Finally, we'll show that prompting the model for 1.-3. can be combined with the formatting instructions covered from our previous guide.\n",
        "\n",
        "We're constantly improving our summarisation capabilities across domains. For more information, you can reach out to us at summarize@cohere.com.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SfEhd3O74--"
      },
      "source": [
        "## Setup\n",
        "\n",
        "You'll need a Cohere API key to run this notebook. If you don't have a key, head to https://cohere.com/ to generate your key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1HzGqnY74bj",
        "outputId": "e822cac4-debf-4f98-d238-0f3f104c44e9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install cohere datasets tokenizers\n",
        "\n",
        "import cohere\n",
        "from getpass import getpass\n",
        "from datasets import load_dataset\n",
        "\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "# Set up Cohere client\n",
        "co_api_key = getpass(\"Enter your Cohere API key: \")\n",
        "co_model = \"command-r\"\n",
        "co = cohere.Client(co_api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbVCPxMSD3fs"
      },
      "outputs": [],
      "source": [
        "# We're also defining some util functions for later\n",
        "\n",
        "def pprint(s: Optional[str] = None, maxchars: int = 100):\n",
        "  \"\"\"\n",
        "  Wrap long text into lines of at most `maxchars` (preserves linebreaks occurring in text)\n",
        "  \"\"\"\n",
        "  if not s:\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\n\".join(line.strip() for line in re.findall(rf\".{{1,{maxchars}}}(?:\\s+|$)\", s)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABgVdI7I8Jmk"
      },
      "source": [
        "## Load test data\n",
        "\n",
        "Let's load a meeting transcript to see Command in action!\n",
        "\n",
        "* If you have your own transcript, you can load it to Colab using your favourite method.\n",
        "* If you don't, we'll use a sample from the [QMSum dataset](https://github.com/Yale-LILY/QMSum). QMSum contains cleaned meeting transcripts with [diarised speakers](https://en.wikipedia.org/wiki/Speaker_diarisation); this will be perfect for testing our model's ability to assign action items to specific speakers.\n",
        "* We'll see later that the recipe shared herein isn't limited to meeting notes transcript, but extends to any data with diarised speakers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS-hEiwQ71lv",
        "outputId": "91343727-fef2-497f-fb98-2e0fbffb90fb"
      },
      "outputs": [],
      "source": [
        "# If you have your own transcript you would like to test Command on, load it here\n",
        "# transcript = ...\n",
        "\n",
        "# Otherwise, we'll use QMSum\n",
        "# Note this will download the QMSum dataset to your instance\n",
        "qmsum = load_dataset(\"MocktaiLEngineer/qmsum-processed\")\n",
        "# Pick any one transcript\n",
        "transcripts = qmsum[\"validation\"][\"meeting_transcript\"]\n",
        "transcript = transcripts[60]\n",
        "pprint(transcript)\n",
        "pprint()\n",
        "\n",
        "# Measure the number of tokens\n",
        "num_tokens = len(co.tokenize(transcript).tokens)\n",
        "pprint(f\"Number of tokens: {num_tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVWdxs3IIqpr"
      },
      "source": [
        "If you've loaded the QMSum dataset, you'll see a back-and-forth dialogue between a Project Manager, an Industrial Designer, and two speakers responsible for Marketing and the User Interface. They seem engaged in a  design discussion with elements of retropsective. Let's see what action items followed from this meeting!\n",
        "\n",
        "Note that if your text length exceeds the context window of Command, you'll need to pre-process that text. Learn more about how to efficiently pre-process long texts at XXX [_include reference_].\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSMTIfnH8xSk"
      },
      "source": [
        "## Build the prompt to extract action items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd3SCjWh9Zh9"
      },
      "source": [
        "To extract action items, no special training is needed with Command!\n",
        "\n",
        "Here's one possible prompt to get action items with the speaker they were assigned to, in the form of bullet points:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAFIK0OP8vwU"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "## meeting transcript\n",
        "{transcript}\n",
        "\n",
        "## instructions\n",
        "Summarize the meeting transcript above, focusing it exclusively around action items. \\\n",
        "Format your answer in the form of bullets. \\\n",
        "Make sure to include the person each action item is assigned to. \\\n",
        "Don't include preambles, postambles or explanations, but respond only with action items.\n",
        "\n",
        "## summary\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PII_STgJ6oz"
      },
      "source": [
        "We applied a few best practices in constructing this prompt:\n",
        "\n",
        "1. We include a preamble imbuing the model with a persona\n",
        "2. We use Markdown-style headers (i.e. with `##`) to delineate the preamble, the text-to-summarise, the instructions, and the output\n",
        "3. We make the instructions specific: we specify that we want action items with their assignees and specify the expected format\n",
        "\n",
        "Experiment with your own prompts, and let us know which worked best for you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2eK_CvN-5Ms"
      },
      "source": [
        "## Generate action items\n",
        "\n",
        "Let's string together the prompt, and generate action items!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zqeRjrI-3Jm",
        "outputId": "a262bf9b-2980-4daf-bb47-aeec4b8fd68d"
      },
      "outputs": [],
      "source": [
        "prompt = prompt_template.format(transcript=transcript)\n",
        "resp = co.chat(message=prompt, model=co_model, temperature=0.3)\n",
        "action_items = resp.text\n",
        "print(action_items)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7taNqRTO2_A"
      },
      "source": [
        "Not bad! We can see that the model successfully retrieved the action items that needed an immediate follow-up, and assigned the correct speaker to them. It also formatted the output as bullet points, as requested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQARVO8RTMh"
      },
      "source": [
        "Sometimes, we need to specify more complex output formats. For instance, we might want to automatically send action items to a company's project management software, which might only accept a certain data format.\n",
        "\n",
        "Say that software accepts only action items as JSON objects where assignees are keys. We could postprocess the previous response into that JSON. Or we could specify those requirements directly into our prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYJ9CPD0ThYT",
        "outputId": "b35e23e1-40e3-489f-a0d7-b1e70e059c71"
      },
      "outputs": [],
      "source": [
        "prompt_template_json = \"\"\"\n",
        "## meeting transcript\n",
        "{transcript}\n",
        "\n",
        "## instructions\n",
        "Summarize the meeting transcript above, focusing it exclusively around action items. \\\n",
        "Format your answer in the form of a compilable JSON, where every speaker is a new key. \\\n",
        "Make sure to include the person each action item is assigned to. \\\n",
        "Don't include preambles, postambles or explanations, but respond only with action items.\n",
        "\n",
        "## summary\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template_json.format(transcript=transcript)\n",
        "resp = co.chat(message=prompt, model=co_model, temperature=0.3)\n",
        "action_items = resp.text\n",
        "print(action_items)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ3X_kJB1mIM"
      },
      "source": [
        "Great! All we needed was to specify the target output format for Command to obey.\n",
        "\n",
        "Try using different instructions to suit Command's outputs to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igIW6yH_ip7_"
      },
      "source": [
        "## How to replicate this pattern for more sub-tasks\n",
        "\n",
        "Command-R readily accommodates changes to the instructions to focus it on other subtasks. As an example, below we adapt the recipe developed for action items to perform two new tasks out-of-the-box:\n",
        "\n",
        "* a. Summarise the meeting from the point of view of every speaker\n",
        "* b. Summarise everything's that been said about a specific topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw9esSWUjIE-"
      },
      "source": [
        "#### a. User perspectives\n",
        "\n",
        "We'll also make the summary extractive, i.e. we encourage the model to reuse passages from the actual meeting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_QSmS5rilHD",
        "outputId": "6013098d-66e7-49ac-b43c-c0a28287e149"
      },
      "outputs": [],
      "source": [
        "prompt_template_perspectives = \"\"\"\n",
        "## meeting transcript\n",
        "{transcript}\n",
        "\n",
        "## instructions\n",
        "Summarize the perspectives of every speaker. \\\n",
        "Format your answer in the form of a JSON, where every speaker is a new key. Don't use your own words, but reuse passages from the meeting transcript where possible. \\\n",
        "Don't include preambles, postambles or explanations, but respond only with your summary of each speaker's perspectives.\n",
        "\n",
        "## summary\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template_perspectives.format(transcript=transcript)\n",
        "resp = co.chat(message=prompt, model=co_model, temperature=0.3)\n",
        "perspectives = resp.text\n",
        "print(perspectives)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ESmupp0jNoa"
      },
      "source": [
        "#### b. Topic focus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-JyBrpDilQ_",
        "outputId": "a24edbe7-47ed-4e88-b8b0-ee4de45c8dba"
      },
      "outputs": [],
      "source": [
        "prompt_template_focus_topic = \"\"\"\\\n",
        "## meeting transcript\n",
        "{transcript}\n",
        "\n",
        "## instructions\n",
        "Summarize everything that's been said about {topic} in the meeting transcript above. If the meeting transcript doesn't mention {topic}, simply state \\\n",
        "that there is no trace of {topic} in the provided meeting transcript. \\\n",
        "Format your answer in the form of paragraphs. \\\n",
        "Don't include preambles, postambles or explanations, but respond only with your summary of {topic}.\n",
        "\n",
        "## summary\n",
        "\"\"\"\n",
        "\n",
        "# Try new topics here!\n",
        "topic = \"objects shaped like bananas\"\n",
        "\n",
        "prompt = prompt_template_focus_topic.format(transcript=transcript, topic=topic)\n",
        "resp = co.chat(message=prompt, model=co_model, temperature=0.3)\n",
        "perspectives = resp.text\n",
        "print(perspectives)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ytrkOUYjPXm"
      },
      "source": [
        "Try some of your own prompts and share them back with us at summarize@cohere.com!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
