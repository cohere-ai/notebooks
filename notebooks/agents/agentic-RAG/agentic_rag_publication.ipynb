{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "## Motivation\n",
    "Asking questions over documents continues to be an important retrieval augmented generation (RAG) task. However, the document complexity can significantly influence overall RAG performance, particularly when the documents are PDFs that contain a mix of text and tables. Finding an optimal strategy to parse this information, chunk, embed and retrieve it is thus quite critical to obtaining accurate results. Furthermore, if the questions being asked over the retrieved documents require mathematical reasoning, then having a model that can validate those operations is quite useful.\n",
    "\n",
    "## Objective\n",
    "In this notebook we will guide you through the best practices of setting up a RAG pipeline to process documents that contain both tables and text. In addition, we will show you how to create a Cohere ReAct agent with access to a RAG pipeline tool to improve accuracy. The general structure of the notebook is as follows:\n",
    "\n",
    "- individual components around parsing, retrieval and generation are covered for documents with mixed tabular and textual data\n",
    "- a class object is created that can be used to instantiate the pipeline with parametric input\n",
    "- the RAG pipeline is then used as a tool for a Cohere ReACT agent\n",
    "\n",
    "# Reference Documents\n",
    "We recommend the following notebook as a guide to [semi-structured RAG](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb).\n",
    "\n",
    "We also recommend the following notebook to explore various parsing techniques for [PDFs](https://github.com/cohere-ai/notebooks/blob/main/notebooks/guides/Document_Parsing_For_Enterprises.ipynb).\n",
    "\n",
    "Various LangChain-supported parsers can be found [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf/).\n",
    "\n",
    "## Table of Contents\n",
    "- Section 1\n",
    "    - [Parsing](#sec_step1)\n",
    "    - [Vector Store Setup](#sec_step2)\n",
    "    - [RAG Pipeline](#sec_step3)\n",
    "- Section 2\n",
    "    - [RAG Pipeline Class](#sec_step4)\n",
    "- Section 3\n",
    "    - [ReAct Agent with RAG Tool](#sec_step5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVT6Sl3msjNe"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0m31LaFDjCIY",
    "outputId": "7177e241-a864-47ed-cf0a-5fc9fa70a7ec"
   },
   "outputs": [],
   "source": [
    "# there may be other dependencies that will need installation\n",
    "# ! pip install --quiet langchain langchain_cohere langchain_experimental\n",
    "# !pip --quiet install faiss-cpu tiktoken\n",
    "# !pip install pypdf\n",
    "# !pip install pytesseract\n",
    "# !pip install opencv-python --upgrade\n",
    "# !pip install \"unstructured[all-docs]\"\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K0GELKJVnadW"
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "from typing import Any\n",
    "import uuid\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import cohere, json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "os.environ['COHERE_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_step1\"></a>\n",
    "# Parsing\n",
    "\n",
    "To improve RAG performance on PDFs with mixed types (text and tables), we investigated a number of parsing and chunking strategies from various libraries:\n",
    "- [PyPDFLoader (LC)](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) (Llama-Index)\n",
    "- [Unstructured](https://unstructured.io/)\n",
    "\n",
    "\n",
    "We have found that the best option for parsing is unstructured.io since the parser can:\n",
    "- separate tables from text\n",
    "- automatically chunk the tables and text by title during the parsing step so that similar elements are grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqcorKP4YEH6"
   },
   "outputs": [],
   "source": [
    "# UNSTRUCTURED pdf loader\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"city_ny_popular_fin_report.pdf\",\n",
    "    # Unstructured first finds embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path='.',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# extract table and textual objects from parser\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_step2\"></a>\n",
    "# Vector Store Setup\n",
    "\n",
    "There are many options for setting up a vector store. Here, we show how to do so using [Chroma](https://www.trychroma.com/) and Langchain's Multi-vector retrieval.\n",
    "As the name implies, multi-vector retrieval allows us to store multiple vectors per document; for instance, for a single document chunk, one could keep embeddings for both the chunk itself, and a summary of that document. A summary may be able to distill more accurately what a chunk is about, leading to better retrieval.\n",
    "\n",
    "You can read more about this here: https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector/\n",
    "\n",
    "Below, we demonstrate the following process:\n",
    "- summaries of each chunk are embedded\n",
    "- during inference, the multi-vector retrieval returns the full context document related to the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "co = cohere.Client()\n",
    "\n",
    "def get_chat_output(message, preamble, chat_history, model, temp, documents=None):\n",
    "    return co.chat(\n",
    "    message=message,\n",
    "    preamble=preamble,\n",
    "    chat_history=chat_history,\n",
    "    documents=documents,\n",
    "    model=model,\n",
    "    temperature=temp\n",
    "    ).text\n",
    "\n",
    "def parallel_proc_chat(prompts,preamble,chat_history=None,model='command-r-plus',temp=0.1,n_jobs=10):\n",
    "    \"\"\"Parallel processing of chat endpoint calls.\"\"\"\n",
    "    responses = Parallel(n_jobs=n_jobs, prefer=\"threads\")(delayed(get_chat_output)(prompt,preamble,chat_history,model,temp) for prompt in prompts)\n",
    "    return responses\n",
    "\n",
    "def rerank_cohere(query, returned_documents,model:str=\"rerank-multilingual-v3.0\",top_n:int=3):\n",
    "    response = co.rerank(\n",
    "        query=query,\n",
    "        documents=returned_documents,\n",
    "        top_n=top_n,\n",
    "        model=model,\n",
    "        return_documents=True\n",
    "    )\n",
    "    top_chunks_after_rerank = [results.document.text for results in response.results]\n",
    "    return top_chunks_after_rerank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate table and text summaries\n",
    "\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "Give a concise summary of the table or text. Table or text chunk: {element}. Only provide the summary and no other text.\"\"\"\n",
    "\n",
    "table_prompts = [prompt_text.format(element=i.text) for i in table_elements]\n",
    "table_summaries = parallel_proc_chat(table_prompts,None)\n",
    "text_prompts = [prompt_text.format(element=i.text) for i in text_elements]\n",
    "text_summaries = parallel_proc_chat(text_prompts,None)\n",
    "tables = [i.text for i in table_elements]\n",
    "texts = [i.text for i in text_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=CohereEmbeddings())\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_step3\"></a>\n",
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our database in place, we can run queries against it. The query process can be broken down into the following steps:\n",
    "\n",
    "- augment the query, this really helps retrieve all the relevant information\n",
    "- use each augmented query to retrieve the top k docs and then rerank them\n",
    "- concatenate all the shortlisted/reranked docs and pass them to the generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, retriever):\n",
    "    \"\"\"Runs query augmentation, retrieval, rerank and final generation in one call.\"\"\"\n",
    "    augmented_queries=co.chat(message=query,model='command-r-plus',temperature=0.2, search_queries_only=True)\n",
    "        #augment queries\n",
    "    if augmented_queries.search_queries:\n",
    "        reranked_docs=[]\n",
    "        for itm in augmented_queries.search_queries:\n",
    "            docs=retriever.invoke(itm.text)\n",
    "            temp_rerank = rerank_cohere(itm.text,docs)\n",
    "            reranked_docs.extend(temp_rerank)\n",
    "        documents = [{\"title\": f\"chunk {i}\", \"snippet\": reranked_docs[i]} for i in range(len(reranked_docs))]\n",
    "    else:\n",
    "        #no queries will be run through RAG\n",
    "        documents = None\n",
    "    \n",
    "    preamble = \"\"\"\n",
    "## Task & Context\n",
    "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
    "\n",
    "## Style Guide\n",
    "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n",
    "\"\"\"\n",
    "    model = 'command-r-plus'\n",
    "    temp = 0.2\n",
    "\n",
    "    \n",
    "    \n",
    "    response = co.chat(\n",
    "      message=query,\n",
    "      documents=documents,\n",
    "      preamble=preamble,\n",
    "      model=model,\n",
    "      temperature=temp\n",
    "    )\n",
    "\n",
    "    final_answer_docs=\"\"\"The final answer is from the documents below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=str(response.documents))\n",
    "\n",
    "    final_answer = response.text\n",
    "    return final_answer, final_answer_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test out a query. In this example, the final answer can be found on page 12 of the PDF, which aligns with the response provided by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The charges for services in 2022 were $5,266 million.\n",
      "The final answer is from the documents below:\n",
      "    \n",
      "    [{'id': 'doc_0', 'snippet': 'Program and General Revenues FY 2023 FY 2022 FY 2021 Category (in millions) Charges for Services (CS) $5,769 $5,266 $5,669 Operating Grants and Contributions (OGC) 27,935 31,757 28,109 Capital Grants and Contributions (CGC) 657 656 675 Real Estate Taxes (RET) 31,502 29,507 31,421 Sales and Use Taxes (SUT) 10,577 10,106 7,614 Personal Income Taxes (PIT) 15,313 15,520 15,795 Income Taxes, Other (ITO) 13,181 9,521 9,499 Other Taxes* (OT) 3,680 3,777 2,755 Investment Income* (II) 694 151 226 Unrestricted Federal and State Aid (UFSA) 234 549 108 Other* (O) Total Program and General Revenues - Primary Government 2,305 $110,250 $107,535 $104,176 708 725', 'title': 'chunk 0'}]\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the charges for services in 2022\"\n",
    "final_answer, final_answer_docs = process_query(query, retriever)\n",
    "print(final_answer)\n",
    "print(final_answer_docs)\n",
    "\n",
    "\n",
    "chat_history=[{'role':\"USER\", 'message':query},{'role':\"CHATBOT\", 'message':f'The final answer is: {final_answer}.' + final_answer_docs}]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat History Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we ask a follow up question that relies on the chat history, but does not require a rerun of the RAG pipeline.\n",
    "\n",
    "We detect questions that do not require RAG by examining the `search_queries` object returned by calling `co.chat` to generate candidate queries to answer our question. If this object is empty, then the model has determined that a document query is not needed to answer the question.\n",
    "\n",
    "In the example below, the `else` statement is invoked based on `query2`. We still pass in the chat history, allowing the question to be answered with only the prior context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG is not needed\n",
      "Final answer:\n",
      "The result of dividing the charges for services in 2022 by two is $2,633.\n"
     ]
    }
   ],
   "source": [
    "query2='divide this by two'\n",
    "augmented_queries=co.chat(message=query2,model='command-r-plus',temperature=0.2, search_queries_only=True)\n",
    "if augmented_queries.search_queries:\n",
    "    print('RAG is needed')\n",
    "    final_answer, final_answer_docs = process_query(query, retriever)\n",
    "    print(final_answer)\n",
    "else:\n",
    "    print('RAG is not needed')\n",
    "    response = co.chat(\n",
    "      message=query2,\n",
    "      model='command-r-plus',\n",
    "      chat_history=chat_history,\n",
    "      temperature=0.3\n",
    "    )\n",
    "    \n",
    "    print(\"Final answer:\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_step4\"></a>\n",
    "# RAG Pipeline Class\n",
    "\n",
    "Here, we connect all of the pieces discussed above into one class object, which is then used as a tool for a Cohere ReAct agent. This class definition consolidates and clarify the key parameters used to define the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "    \n",
    "class RAG_pipeline():\n",
    "    def __init__(self,paths):\n",
    "        self.embedding_model=\"embed-english-v3.0\"\n",
    "        self.generation_model=\"command-r-plus\"\n",
    "        self.summary_model=\"command-r-plus\"\n",
    "        self.rerank_model=\"rerank-multilingual-v3.0\"\n",
    "        self.num_docs_to_retrieve = 10\n",
    "        self.top_k_rerank=3\n",
    "        self.temperature=0.2\n",
    "        self.preamble=\"\"\"\n",
    "## Task & Context\n",
    "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
    "\n",
    "## Style Guide\n",
    "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n",
    "\"\"\"     \n",
    "        self.n_jobs=10 #number of parallel processes to run summarization of chunks\n",
    "        self.extract_images_in_pdf=False\n",
    "        self.infer_table_structure=True\n",
    "        self.chunking_strategy=\"by_title\"\n",
    "        self.max_characters=4000\n",
    "        self.new_after_n_chars=3800\n",
    "        self.combine_text_under_n_chars=2000\n",
    "        self.image_output_dir_path='.'\n",
    "        self.paths = paths\n",
    "        self.parse_and_build_retriever()\n",
    "        \n",
    "    def parse_and_build_retriever(self,):\n",
    "        #step1, parse pdfs\n",
    "        # if condition just for debugging since perf_audit.pdf is parsed in the prev step, no need to rerun\n",
    "        parsed_pdf_list=self.parse_pdfs(self.paths)\n",
    "        #separate tables and text\n",
    "        extracted_tables, extracted_text = self.extract_text_and_tables(parsed_pdf_list)\n",
    "        #generate summaries for everything\n",
    "        tables, table_summaries, texts, text_summaries=self.generate_summaries(extracted_tables,extracted_text)\n",
    "        self.tables = tables\n",
    "        self.table_summaries = table_summaries\n",
    "        self.texts = texts\n",
    "        self.text_summaries=text_summaries\n",
    "        #setup the multivector retriever\n",
    "        self.make_retriever(tables, table_summaries, texts, text_summaries)\n",
    "        \n",
    "    def extract_text_and_tables(self,parsed_pdf_list):\n",
    "        # extract table and textual objects from parser\n",
    "        # Categorize by type\n",
    "        all_table_elements = []\n",
    "        all_text_elements = []\n",
    "        for raw_pdf_elements in parsed_pdf_list:\n",
    "            categorized_elements = []\n",
    "            for element in raw_pdf_elements:\n",
    "                if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "                    categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "                elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "                    categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "            \n",
    "            # Tables\n",
    "            table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "            print(len(table_elements))\n",
    "            \n",
    "            # Text\n",
    "            text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "            print(len(text_elements))\n",
    "            all_table_elements.extend(table_elements)\n",
    "            all_text_elements.extend(text_elements)\n",
    "\n",
    "        return all_table_elements, all_text_elements\n",
    "            \n",
    "    def parse_pdfs(self, paths):\n",
    "\n",
    "        path_raw_elements = []\n",
    "        for path in paths:\n",
    "            raw_pdf_elements = partition_pdf(\n",
    "            filename=path,\n",
    "            # Unstructured first finds embedded image blocks\n",
    "            extract_images_in_pdf=self.extract_images_in_pdf,\n",
    "            # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "            # Titles are any sub-section of the document\n",
    "            infer_table_structure=self.infer_table_structure,\n",
    "            # Post processing to aggregate text once we have the title\n",
    "            chunking_strategy=self.chunking_strategy,\n",
    "            # Chunking params to aggregate text blocks\n",
    "            # Attempt to create a new chunk 3800 chars\n",
    "            # Attempt to keep chunks > 2000 chars\n",
    "            max_characters=self.max_characters,\n",
    "            new_after_n_chars=self.new_after_n_chars,\n",
    "            combine_text_under_n_chars=self.combine_text_under_n_chars,\n",
    "            image_output_dir_path=self.image_output_dir_path,\n",
    "            )\n",
    "            path_raw_elements.append(raw_pdf_elements)\n",
    "        print('PDFs parsed')\n",
    "        return path_raw_elements\n",
    "        \n",
    "\n",
    "    def get_chat_output(self,message, preamble, model, temp):\n",
    "        # print(\"**message\")\n",
    "        # print(message)\n",
    "        \n",
    "        response=co.chat(\n",
    "            message=message,\n",
    "            preamble=preamble,\n",
    "            model=model,\n",
    "            temperature=temp\n",
    "            ).text\n",
    "        # print(\"**output\")\n",
    "        # print(response)\n",
    "        return response\n",
    "\n",
    "    def parallel_proc_chat(self,prompts,preamble,model,temp,n_jobs):\n",
    "        \"\"\"Parallel processing of chat endpoint calls.\"\"\"\n",
    "        responses = Parallel(n_jobs=n_jobs, prefer=\"threads\")(delayed(self.get_chat_output)(prompt,preamble,model,temp) for prompt in prompts)\n",
    "        return responses\n",
    "    \n",
    "    def rerank_cohere(self,query, returned_documents,model, top_n):\n",
    "        response = co.rerank(\n",
    "            query=query,\n",
    "            documents=returned_documents,\n",
    "            top_n=top_n,\n",
    "            model=model,\n",
    "            return_documents=True\n",
    "        )\n",
    "        top_chunks_after_rerank = [results.document.text for results in response.results]\n",
    "        return top_chunks_after_rerank\n",
    "\n",
    "    def generate_summaries(self,table_elements,text_elements):\n",
    "        # generate table and text summaries\n",
    "\n",
    "        summarize_prompt = \"\"\"You are an assistant tasked with summarizing tables and text. \\ \n",
    "        Give a concise summary of the table or text. Table or text chunk: {element}. Only provide the summary and no other text.\"\"\"\n",
    "        \n",
    "        table_prompts = [summarize_prompt.format(element=i.text) for i in table_elements]\n",
    "        table_summaries = self.parallel_proc_chat(table_prompts,self.preamble,self.summary_model,self.temperature,self.n_jobs)\n",
    "        text_prompts = [summarize_prompt.format(element=i.text) for i in text_elements]\n",
    "        text_summaries = self.parallel_proc_chat(text_prompts,self.preamble,self.summary_model,self.temperature,self.n_jobs)\n",
    "        tables = [i.text for i in table_elements]\n",
    "        texts = [i.text for i in text_elements]\n",
    "        print('summaries generated')\n",
    "        return tables, table_summaries, texts, text_summaries\n",
    "\n",
    "    def make_retriever(self,tables, table_summaries, texts, text_summaries):\n",
    "        # The vectorstore to use to index the child chunks\n",
    "        vectorstore = Chroma(collection_name=\"summaries\", embedding_function=CohereEmbeddings())\n",
    "        # The storage layer for the parent documents\n",
    "        store = InMemoryStore()\n",
    "        id_key = \"doc_id\"\n",
    "        # The retriever (empty to start)\n",
    "        retriever = MultiVectorRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            docstore=store,\n",
    "            id_key=id_key,\n",
    "            search_kwargs={\"k\": self.num_docs_to_retrieve}\n",
    "        )\n",
    "        # Add texts\n",
    "        doc_ids = [f'text_{i}' for i in range(len(texts))]#[str(uuid.uuid4()) for _ in texts]\n",
    "        summary_texts = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(text_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_texts,ids=doc_ids)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "        # Add tables\n",
    "        table_ids = [f'table_{i}' for i in range(len(texts))]#[str(uuid.uuid4()) for _ in tables]\n",
    "        summary_tables = [\n",
    "            Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "            for i, s in enumerate(table_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_tables,ids=table_ids)\n",
    "        retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "        self.retriever = retriever\n",
    "        print('retriever built')\n",
    "        \n",
    "    def process_query(self,query):\n",
    "        \"\"\"Runs query augmentation, retrieval, rerank and generation in one call.\"\"\"\n",
    "        augmented_queries=co.chat(message=query,model=self.generation_model,temperature=self.temperature, search_queries_only=True)\n",
    "        #augment queries\n",
    "        if augmented_queries.search_queries:\n",
    "            reranked_docs=[]\n",
    "            for itm in augmented_queries.search_queries:\n",
    "                docs=self.retriever.invoke(itm.text)\n",
    "                temp_rerank = self.rerank_cohere(itm.text,docs,model=self.rerank_model,top_n=self.top_k_rerank)\n",
    "                reranked_docs.extend(temp_rerank)\n",
    "            documents = [{\"title\": f\"chunk {i}\", \"snippet\": reranked_docs[i]} for i in range(len(reranked_docs))]\n",
    "        else:\n",
    "            documents = None\n",
    "            \n",
    "        response = co.chat(\n",
    "          message=query,\n",
    "          documents=documents,\n",
    "          preamble=self.preamble,\n",
    "          model=self.generation_model,\n",
    "          temperature=self.temperature\n",
    "        )\n",
    "    \n",
    "        final_answer_docs=\"\"\"The final answer is from the documents below:\n",
    "        \n",
    "        {docs}\"\"\".format(docs=str(response.documents))\n",
    "    \n",
    "        final_answer = response.text\n",
    "        return final_answer, final_answer_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs parsed\n",
      "14\n",
      "24\n",
      "summaries generated\n",
      "retriever built\n"
     ]
    }
   ],
   "source": [
    "rag_object=RAG_pipeline(paths=[\"city_ny_popular_fin_report.pdf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec_step5\"></a>\n",
    "# Cohere ReAct Agent with RAG Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a simple agent that utilizes the RAG pipeline defined above. We do this by granting the agent access to two tools:\n",
    "\n",
    "- the end-to-end RAG pipeline \n",
    "- a Python interpreter\n",
    "\n",
    "The intention behind coupling these tools is to enable the model to perform mathematical and other postprocessing operations on RAG outputs using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_cohere.react_multi_hop.agent import create_cohere_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_cohere.chat_models import ChatCohere\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "class react_agent():\n",
    "    def __init__(self,rag_retriever,model=\"command-r-plus\",temperature=0.2):\n",
    "        self.llm = ChatCohere(model=model, temperature=temperature)\n",
    "        self.preamble=\"\"\"\n",
    "## Task & Context\n",
    "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
    "\n",
    "## Style Guide\n",
    "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n",
    "\n",
    "## Guidelines\n",
    "You are an expert who answers the user's question. \n",
    "You have access to a vectorsearch tool that will use your query to search through documents and find the relevant answer.\n",
    "You also have access to a python interpreter tool which you can use to run code for mathematical operations.\n",
    "\"\"\"\n",
    "        self.get_tools(rag_retriever)\n",
    "        self.build_agent()\n",
    "        \n",
    "    def get_tools(self,rag_retriever):\n",
    "        @tool\n",
    "        def vectorsearch(query: str):\n",
    "            \"\"\"Uses the query to search through a list of documents and return the most relevant documents as well as the answer.\"\"\"\n",
    "            final_answer, final_answer_docs=rag_retriever.process_query(query)\n",
    "            return final_answer + final_answer_docs\n",
    "        vectorsearch.name = \"vectorsearch\" # use python case\n",
    "        vectorsearch.description = \"Uses the query to search through a list of documents and return the most relevant documents as well as the answer.\"\n",
    "        class vectorsearch_inputs(BaseModel):\n",
    "            query: str = Field(description=\"the users query\")\n",
    "        vectorsearch.args_schema = vectorsearch_inputs\n",
    "\n",
    "        \n",
    "        python_repl = PythonREPL()\n",
    "        python_tool = Tool(\n",
    "            name=\"python_repl\",\n",
    "            description=\"Executes python code and returns the result. The code runs in a static sandbox without interactive mode, so print output or save output to a file.\",\n",
    "            func=python_repl.run,\n",
    "        )\n",
    "        python_tool.name = \"python_interpreter\"\n",
    "        class ToolInput(BaseModel):\n",
    "            code: str = Field(description=\"Python code to execute.\")\n",
    "        python_tool.args_schema = ToolInput\n",
    "\n",
    "        self.alltools = [vectorsearch,python_tool]\n",
    "\n",
    "    def build_agent(self):\n",
    "        # Prompt template\n",
    "        prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "        # Create the ReAct agent\n",
    "        agent = create_cohere_react_agent(\n",
    "            llm=self.llm,\n",
    "            tools=self.alltools,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        self.agent_executor = AgentExecutor(agent=agent, tools=self.alltools, verbose=True,return_intermediate_steps=True)\n",
    "\n",
    "\n",
    "    def run_agent(self,query,history=None):\n",
    "        if history:\n",
    "            response=self.agent_executor.invoke({\n",
    "            \"input\": query,\n",
    "            \"preamble\": self.preamble,\n",
    "            \"chat_history\": history\n",
    "        })\n",
    "        else:\n",
    "            response=self.agent_executor.invoke({\n",
    "            \"input\": query,\n",
    "            \"preamble\": self.preamble,\n",
    "        })\n",
    "        return response\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_object=react_agent(rag_retriever=rag_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "I will search for the charges for services in 2022 and 2023.\n",
      "{'tool_name': 'vectorsearch', 'parameters': {'query': 'charges for services in 2022 and 2023'}}\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.The final answer is from the documents below:\n",
      "        \n",
      "        [{'id': 'doc_0', 'snippet': 'Program and General Revenues FY 2023 FY 2022 FY 2021 Category (in millions) Charges for Services (CS) $5,769 $5,266 $5,669 Operating Grants and Contributions (OGC) 27,935 31,757 28,109 Capital Grants and Contributions (CGC) 657 656 675 Real Estate Taxes (RET) 31,502 29,507 31,421 Sales and Use Taxes (SUT) 10,577 10,106 7,614 Personal Income Taxes (PIT) 15,313 15,520 15,795 Income Taxes, Other (ITO) 13,181 9,521 9,499 Other Taxes* (OT) 3,680 3,777 2,755 Investment Income* (II) 694 151 226 Unrestricted Federal and State Aid (UFSA) 234 549 108 Other* (O) Total Program and General Revenues - Primary Government 2,305 $110,250 $107,535 $104,176 708 725', 'title': 'chunk 0'}]\u001b[0m\u001b[32;1m\u001b[1;3mRelevant Documents: 0\n",
      "Cited Documents: 0\n",
      "Answer: The charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.\n",
      "Grounded answer: The charges for services in <co: 0>2022</co: 0> were <co: 0>$5,266 million</co: 0> and in <co: 0>2023</co: 0> were <co: 0>$5,769 million</co: 0>.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "step1_response=agent_object.run_agent(\"what are the charges for services in 2022 and 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like earlier, we can also pass chat history to the LangChain agent to refer to for any other queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[\n",
    "HumanMessage(content=step1_response['input']),\n",
    "AIMessage(content=step1_response['output'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "I will use the Python Interpreter tool to calculate the mean of the two values.\n",
      "{'tool_name': 'python_interpreter', 'parameters': {'code': 'import numpy as np\\n\\n# Data\\nvalues = [5266, 5769]\\n\\n# Calculate the mean\\nmean_value = np.mean(values)\\n\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")'}}\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThe mean of the two values is: 5518 million\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mRelevant Documents: 0\n",
      "Cited Documents: 0\n",
      "Answer: The mean of the two values is 5518 million.\n",
      "Grounded answer: The mean of the two values is <co: 0>5518 million</co: 0>.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the mean of the two values',\n",
       " 'preamble': \"\\n## Task & Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\n\\n## Guidelines\\nYou are an expert who answers the user's question. \\nYou have access to a vectorsearch tool that will use your query to search through documents and find the relevant answer.\\nYou also have access to a python interpreter tool which you can use to run code for mathematical operations.\\n\",\n",
       " 'chat_history': [HumanMessage(content='what are the charges for services in 2022 and 2023'),\n",
       "  AIMessage(content='The charges for services in 2022 were $5,266 million and in 2023 were $5,769 million.')],\n",
       " 'output': 'The mean of the two values is 5518 million.',\n",
       " 'citations': [CohereCitation(start=30, end=42, text='5518 million', documents=[{'output': 'The mean of the two values is: 5518 million\\n'}])],\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='python_interpreter', tool_input={'code': 'import numpy as np\\n\\n# Data\\nvalues = [5266, 5769]\\n\\n# Calculate the mean\\nmean_value = np.mean(values)\\n\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")'}, log='\\nI will use the Python Interpreter tool to calculate the mean of the two values.\\n{\\'tool_name\\': \\'python_interpreter\\', \\'parameters\\': {\\'code\\': \\'import numpy as np\\\\n\\\\n# Data\\\\nvalues = [5266, 5769]\\\\n\\\\n# Calculate the mean\\\\nmean_value = np.mean(values)\\\\n\\\\nprint(f\"The mean of the two values is: {mean_value:.0f} million\")\\'}}\\n', message_log=[AIMessage(content='\\nPlan: I will use the Python Interpreter tool to calculate the mean of the two values.\\nAction: ```json\\n[\\n    {\\n        \"tool_name\": \"python_interpreter\",\\n        \"parameters\": {\\n            \"code\": \"import numpy as np\\\\n\\\\n# Data\\\\nvalues = [5266, 5769]\\\\n\\\\n# Calculate the mean\\\\nmean_value = np.mean(values)\\\\n\\\\nprint(f\\\\\"The mean of the two values is: {mean_value:.0f} million\\\\\")\"\\n        }\\n    }\\n]\\n```')]),\n",
       "   'The mean of the two values is: 5518 million\\n')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_object.run_agent(\"what is the mean of the two values\",history=chat_history)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
