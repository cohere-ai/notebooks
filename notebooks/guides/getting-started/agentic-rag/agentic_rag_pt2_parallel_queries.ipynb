{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/guides/agentic-rag/agentic_rag_pt2_parallel_queries.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Parallel Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare two user queries to a RAG chatbot, \"What was Apple's revenue in 2023?\" and \"What were Apple's and Google's revenue in 2023?\".\n",
    "\n",
    "The first query is straightforward as you can perform retrieval using pretty much the same query you get.\n",
    "\n",
    "But the second query is more complex. We need to break it down into two separate queries, one for Apple and one for Google.\n",
    "\n",
    "This is an example that requires parallel query execution. Here, the agentic RAG will need to turn or expand the query it gets from the user into a more optimized set of queries it should use to perform the retrieval.\n",
    "\n",
    "In this part, you will learn how agentic RAG with Cohere handles parallel query execution.\n",
    "- Query expansion\n",
    "- Query expansion over multiple data sources\n",
    "- Query expansion in multi-turn conversations\n",
    "\n",
    "\n",
    "You'll learn these by building an agent that answers questions about using Cohere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, first we need to install the `cohere` library and create a Cohere client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cohere\n",
    "\n",
    "from tool_def import (\n",
    "    search_developer_docs,\n",
    "    search_developer_docs_tool,\n",
    "    search_internet,\n",
    "    search_internet_tool,\n",
    "    search_code_examples,\n",
    "    search_code_examples_tool,\n",
    ")\n",
    "\n",
    "co = cohere.ClientV2(api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the same set of tools as in Part 1. If you want further details on how to set up the tools, check out Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_internet\": search_internet,\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an agentic RAG workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `run_agent` function to run the agentic RAG workflow, the same as in Part 1. If you want further details on how to set up the tools, check out Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    search_developer_docs_tool,\n",
    "    search_internet_tool,\n",
    "    search_code_examples_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"\"\"## Task and Context\n",
    "You are an assistant who helps developers use Cohere. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-r-plus-08-2024\"\n",
    "\n",
    "def run_agent(query, messages=None):\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "        \n",
    "    if \"system\" not in {m.get(\"role\") for m in messages}:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Step 1: get user message\n",
    "    print(f\"QUESTION:\\n{query}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    while response.message.tool_calls:\n",
    "        \n",
    "        print(\"TOOL PLAN:\")\n",
    "        print(response.message.tool_plan,\"\\n\")\n",
    "        print(\"TOOL CALLS:\")\n",
    "        for tc in response.message.tool_calls:\n",
    "            print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": response.message.tool_calls, \"tool_plan\": response.message.tool_plan})        \n",
    "        \n",
    "        # Step 3: Get tool results\n",
    "        for tc in response.message.tool_calls:\n",
    "            tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "            tool_content = []\n",
    "            for data in tool_result:\n",
    "                tool_content.append({\"type\": \"document\", \"document\": {\"data\": json.dumps(data)}})\n",
    "                # Optional: add an \"id\" field in the \"document\" object, otherwise IDs are auto-generated\n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content})\n",
    "        \n",
    "        # Step 4: Generate response and citations \n",
    "        response = co.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
    "        \n",
    "    # Print final response\n",
    "    print(\"RESPONSE:\")\n",
    "    print(response.message.content[0].text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print citations (if any)\n",
    "    verbose_source = False # Change to True to display the contents of a source\n",
    "    if response.message.citations:\n",
    "        print(\"CITATIONS:\\n\")\n",
    "        for citation in response.message.citations:\n",
    "            print(f\"Start: {citation.start}| End:{citation.end}| Text:'{citation.text}' \")\n",
    "            print(\"Sources:\")\n",
    "            for idx, source in enumerate(citation.sources):\n",
    "                print(f\"{idx+1}. {source.id}\")\n",
    "                if verbose_source:\n",
    "                    print(f\"{source.tool_output}\")\n",
    "            print(\"\\n\")\n",
    "                    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the agent a few questions, starting with this one about the Chat endpoint and the RAG feature.\n",
    "\n",
    "Firstly, the agent rightly chooses the `search_developer_docs` tool to retrieve the information it needs.\n",
    "\n",
    "Additionally, because the question asks about two different things, retrieving information using the same query as the user's may not be the most optimal. Instead, the query needs to be expanded, or split into multiple parts, each retrieving its own set of documents.\n",
    "\n",
    "Thus, the agent expands the original query into two queries.\n",
    "\n",
    "This is enabled by the parallel tool calling feature that comes with the Chat endpoint.\n",
    "\n",
    "This results in a richer and more representative list of documents retrieved, and therefore a more accurate and comprehensive answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Explain the Chat endpoint and the RAG feature\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'Chat endpoint' and 'RAG feature' to find the information I need to answer the question. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"Chat endpoint\"}\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"RAG feature\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The Chat endpoint facilitates a conversational interface, allowing users to send messages to the model and receive text responses.\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a method for generating text using additional information fetched from an external data source, which can greatly increase the accuracy of the response.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 4| End:17| Text:'Chat endpoint' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:3\n",
      "2. search_developer_docs_0tyvan5t0f7t:3\n",
      "\n",
      "\n",
      "Start: 32| End:56| Text:'conversational interface' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:3\n",
      "2. search_developer_docs_0tyvan5t0f7t:3\n",
      "\n",
      "\n",
      "Start: 67| End:102| Text:'users to send messages to the model' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:3\n",
      "2. search_developer_docs_0tyvan5t0f7t:3\n",
      "\n",
      "\n",
      "Start: 107| End:130| Text:'receive text responses.' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:3\n",
      "2. search_developer_docs_0tyvan5t0f7t:3\n",
      "\n",
      "\n",
      "Start: 132| End:162| Text:'Retrieval Augmented Generation' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:4\n",
      "2. search_developer_docs_0tyvan5t0f7t:4\n",
      "\n",
      "\n",
      "Start: 163| End:168| Text:'(RAG)' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:4\n",
      "2. search_developer_docs_0tyvan5t0f7t:4\n",
      "\n",
      "\n",
      "Start: 174| End:200| Text:'method for generating text' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:4\n",
      "2. search_developer_docs_0tyvan5t0f7t:4\n",
      "\n",
      "\n",
      "Start: 207| End:266| Text:'additional information fetched from an external data source' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:4\n",
      "2. search_developer_docs_0tyvan5t0f7t:4\n",
      "\n",
      "\n",
      "Start: 278| End:324| Text:'greatly increase the accuracy of the response.' \n",
      "Sources:\n",
      "1. search_developer_docs_7qmwn64d2e8v:4\n",
      "2. search_developer_docs_0tyvan5t0f7t:4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Explain the Chat endpoint and the RAG feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query expansion over multiple data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier example focused on a single data source, the Cohere developer documentation. However, the agentic RAG can also perform query expansion over multiple data sources.\n",
    "\n",
    "Here, the agent is asked a question that contains two parts: first asking for an explanation of the Embed endpoint and then asking for code examples.\n",
    "\n",
    "It correctly identifies that this requires both searching the developer documentation and the code examples. Thus, it generates two queries, one for each data source, and performs two separate searches in parallel.\n",
    "\n",
    "Its response then contains information referenced from both data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "What is the Embed endpoint? Give me some code tutorials\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'What is the Embed endpoint?' and 'code tutorials for Embed endpoint'. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"What is the Embed endpoint?\"}\n",
      "Tool name: search_code_examples | Parameters: {\"query\":\"code tutorials for Embed endpoint\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The Embed endpoint returns text embeddings. An embedding is a list of floating point numbers that captures semantic information about the text that it represents.\n",
      "\n",
      "In addition to embed-english-v3.0, we offer a best-in-class multilingual model embed-multilingual-v3.0 with support for over 100 languages.\n",
      "\n",
      "Here are some code tutorials for the Embed endpoint:\n",
      "- Wikipedia Semantic Search with Cohere Embedding Archives\n",
      "- RAG With Chat Embed and Rerank via Pinecone\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 19| End:43| Text:'returns text embeddings.' \n",
      "Sources:\n",
      "1. search_developer_docs_n6n5ka20389e:1\n",
      "\n",
      "\n",
      "Start: 47| End:162| Text:'embedding is a list of floating point numbers that captures semantic information about the text that it represents.' \n",
      "Sources:\n",
      "1. search_developer_docs_n6n5ka20389e:1\n",
      "\n",
      "\n",
      "Start: 179| End:197| Text:'embed-english-v3.0' \n",
      "Sources:\n",
      "1. search_developer_docs_n6n5ka20389e:2\n",
      "\n",
      "\n",
      "Start: 210| End:303| Text:'best-in-class multilingual model embed-multilingual-v3.0 with support for over 100 languages.' \n",
      "Sources:\n",
      "1. search_developer_docs_n6n5ka20389e:2\n",
      "\n",
      "\n",
      "Start: 360| End:416| Text:'Wikipedia Semantic Search with Cohere Embedding Archives' \n",
      "Sources:\n",
      "1. search_code_examples_680znd4ycmm3:1\n",
      "\n",
      "\n",
      "Start: 419| End:462| Text:'RAG With Chat Embed and Rerank via Pinecone' \n",
      "Sources:\n",
      "1. search_code_examples_680znd4ycmm3:2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What is the Embed endpoint? Give me some code tutorials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query expansion in multi-turn conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RAG chatbot needs to be able to infer the user's intent for a given query, sometimes based on a vague context.\n",
    "\n",
    "This is especially important in multi-turn conversations, where the user's intent may not be clear from a single query. \n",
    "\n",
    "For example, in the first turn, a user might ask \"What is A\" and in the second turn, they might ask \"Compare that with B and C\". So, the agent needs to be able to infer that the user's intent is to compare A with B and C.\n",
    "\n",
    "Let's see an example of this. First, note that the `run_agent` function is already enabled to handle multi-turn conversations. It can take messages from the previous conversation turns and append them to the `messages` list.\n",
    "\n",
    "So in the first turn, the user asks about the Chat endpoint, which the agent duly provides a response to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "What is the Chat endpoint?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'What is the Chat endpoint?' \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"What is the Chat endpoint?\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The Chat endpoint facilitates a conversational interface, allowing users to send messages to the model and receive text responses.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 18| End:130| Text:'facilitates a conversational interface, allowing users to send messages to the model and receive text responses.' \n",
      "Sources:\n",
      "1. search_developer_docs_91yqedvwtgkj:3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What is the Chat endpoint?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second turn, the user asks a question that contains two parts: first asking how it's different from RAG and then asking for code examples.\n",
    "\n",
    "We pass the messages from the previous conversation turn to the `run_agent` function.\n",
    "\n",
    "Because of this, the agent is able to infer that the question is referring to the Chat endpoint even though the user didn't explicitly mention it.\n",
    "\n",
    "And the agent goes on to expand the query into two separate queries, one for the `search_code_examples` tool and one for the `search_developer_docs` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "How is it different from RAG? Also any code tutorials?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'How is Chat endpoint different from RAG?' and 'Chat endpoint code tutorials'. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"How is Chat endpoint different from RAG?\"}\n",
      "Tool name: search_code_examples | Parameters: {\"query\":\"Chat endpoint code tutorials\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The Chat endpoint facilitates a conversational interface, allowing users to send messages to the model and receive text responses. Retrieval Augmented Generation (RAG) is a method for generating text using additional information fetched from an external data source, which can greatly increase the accuracy of the response.\n",
      "\n",
      "Here are some code tutorials:\n",
      "- RAG with Chat Embed and Rerank via Pinecone\n",
      "- Build chatbots that know your business with MongoDB and Cohere\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 32| End:56| Text:'conversational interface' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:3\n",
      "\n",
      "\n",
      "Start: 76| End:102| Text:'send messages to the model' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:3\n",
      "\n",
      "\n",
      "Start: 107| End:130| Text:'receive text responses.' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:3\n",
      "\n",
      "\n",
      "Start: 131| End:161| Text:'Retrieval Augmented Generation' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:4\n",
      "\n",
      "\n",
      "Start: 162| End:167| Text:'(RAG)' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:4\n",
      "\n",
      "\n",
      "Start: 184| End:199| Text:'generating text' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:4\n",
      "\n",
      "\n",
      "Start: 206| End:265| Text:'additional information fetched from an external data source' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:4\n",
      "\n",
      "\n",
      "Start: 277| End:323| Text:'greatly increase the accuracy of the response.' \n",
      "Sources:\n",
      "1. search_developer_docs_93b1gz27dq9d:4\n",
      "\n",
      "\n",
      "Start: 357| End:400| Text:'RAG with Chat Embed and Rerank via Pinecone' \n",
      "Sources:\n",
      "1. search_code_examples_qj3q45zxk8gz:2\n",
      "\n",
      "\n",
      "Start: 403| End:465| Text:'Build chatbots that know your business with MongoDB and Cohere' \n",
      "Sources:\n",
      "1. search_code_examples_qj3q45zxk8gz:3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"How is it different from RAG? Also any code tutorials?\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about:\n",
    "- How query expansion works in an agentic RAG system\n",
    "- How query expansion works over multiple data sources\n",
    "- How query expansion works in multi-turn conversations\n",
    "\n",
    "Having said that, we may encounter even more complex queries that what we've seen so far. In particular, there are queries that require sequential reasoning where the retrieval needs to happen over multiple steps.\n",
    "\n",
    "In Part 3, you will learn how the agentic RAG system can perform sequential reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
