{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command is Cohere’s flagship LLM. It generates a response based on a user message or prompt. It is trained to follow user commands and to be instantly useful in practical business applications, like summarization, copywriting, extraction, and question-answering.\n",
    "\n",
    "Command R and Command R+ are the most recent models in the Command family. They are the market-leading models that balance high efficiency with strong accuracy to enable enterprises to move from proof of concept into production-grade AI.\n",
    "\n",
    "You'll use Chat, the Cohere endpoint for accessing the Command models.\n",
    "\n",
    "In this tutorial, you'll learn about:\n",
    "- Basic text generation\n",
    "- Prompt engineering\n",
    "- Parameters for controlling output\n",
    "- Structured output generation\n",
    "- Streamed output\n",
    "\n",
    "You'll learn these by building an onboarding assistant for new hires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, first we need to install the `cohere` library and create a Cohere client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cohere\n",
    "\n",
    "import cohere\n",
    "import json\n",
    "\n",
    "co = cohere.ClientV2(api_key=\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with Chat, we need to pass two parameters, `model` for the LLM model ID and `messages`, which we add a single user message. We then call the Chat endpoint through the client we created earlier.\n",
    "\n",
    "The response contains several objects. For simplicity, what we want right now is the `message.content[0].text` object.\n",
    "\n",
    "Here's an example of the assistant responding to a new hire's query asking for help to make introductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is a draft of an introduction message: \n",
      "\n",
      "\"Hi everyone! My name is [Your Name], and I am thrilled to be joining the Co1t team today. I am excited to get to know you all and contribute to the amazing work being done at this startup. A little about me: [Brief description of your role, experience, and interests]. Outside of work, I enjoy [Hobbies and interests]. I look forward to collaborating with you all and being a part of Co1t's journey. Let's connect and make something great together!\" \n",
      "\n",
      "Feel free to edit and personalize the message to your liking. Good luck with your new role at Co1t!\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
    "\n",
    "# Generate the response\n",
    "response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
    "                #    messages=[cohere.UserMessage(content=message)])\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Chat endpoint API reference](https://docs.cohere.com/v2/reference/chat)\n",
    "- [Documentation on Chat fine-tuning](https://docs.cohere.com/docs/chat-fine-tuning)\n",
    "- [Documentation on Command R+](https://docs.cohere.com/docs/command-r-plus)\n",
    "- [LLM University module on text generation](https://cohere.com/llmu#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting is at the heart of working with LLMs. The prompt provides context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output.\n",
    "\n",
    "In this section, we'll look at a couple of prompting techniques.\n",
    "\n",
    "The first is to add more specific instructions to the prompt. The more instructions you provide in the prompt, the closer you can get to the response you need.\n",
    "\n",
    "The limit of how long a prompt can be is dependent on the maximum context length that a model can support (in the case Command R/R+, it's 128k tokens).\n",
    "\n",
    "Below, we'll add one additional instruction to the earlier prompt: the length we need the response to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hi everyone, my name is [Your Name], and I am thrilled to join the Co1t team today as a [Your Role], eager to contribute my skills and ideas to the company's growth and success!\"\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
    "\n",
    "# Generate the response\n",
    "response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
    "                #    messages=[cohere.UserMessage(content=message)])\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our prompts so far use what is called zero-shot prompting, which means that provide instruction without any example. But in many cases, it is extremely helpful to provide examples to the model to guide its response. This is called few-shot prompting.\n",
    "\n",
    "Few-shot prompting is especially useful when we want the model response to follow a particular style or format. Also, it is sometimes hard to explain what you want in an instruction, and easier to show examples.\n",
    "\n",
    "Below, we want the response to be similar in style and length to the convention, as we show in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket title: \"Server Access Permissions Issue\"\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "user_input = \"Why can't I access the server? Is it a permissions issue?\"\n",
    "\n",
    "# Create a prompt containing example outputs\n",
    "message=f\"\"\"Write a ticket title for the following user request:\n",
    "\n",
    "User request: Where are the usual storage places for project files?\n",
    "Ticket title: Project File Storage Location\n",
    "\n",
    "User request: Emails won't send. What could be the issue?\n",
    "Ticket title: Email Sending Issues\n",
    "\n",
    "User request: How can I set up a connection to the office printer?\n",
    "Ticket title: Printer Connection Setup\n",
    "\n",
    "User request: {user_input}\n",
    "Ticket title:\"\"\"\n",
    "\n",
    "# Generate the response\n",
    "response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Documentation on prompt engineering](https://docs.cohere.com/docs/crafting-effective-prompts)\n",
    "- [LLM University module on prompt engineering](https://cohere.com/llmu#prompt-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for controlling output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chat endpoint provides developers with an array of options and parameters.\n",
    "\n",
    "For example, you can choose from several variations of the Command model. Different models produce different output profiles, such as quality and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hi, I'm [Your Name] and I'm thrilled to join the Co1t team today as a [Your Role], eager to contribute my skills and ideas to help drive innovation and success for our startup!\"\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
    "\n",
    "# Generate the response\n",
    "response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
    "\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you’ll need to control the level of randomness of the output. You can control this using a few parameters.\n",
    "\n",
    "The most commonly used parameter is `temperature`, which is a number used to tune the degree of randomness. You can enter values between 0.0 to 1.0.\n",
    "\n",
    "A lower temperature gives more predictable outputs, and a higher temperature gives more \"creative\" outputs.\n",
    "\n",
    "Here's an example of setting `temperature` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \"Revolution Enthusiast\"\n",
      "\n",
      "2: \"Revolution Enthusiast\"\n",
      "\n",
      "3: \"Revolution Enthusiast\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I like learning about the industrial revolution and how it shapes the modern world. How I can introduce myself in five words or less.\"\n",
    "\n",
    "# Generate the response multiple times by specifying a low temperature value\n",
    "for idx in range(3):\n",
    "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    temperature=0)\n",
    "\n",
    "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an example of setting `temperature` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Here is a suggestion: \n",
      "\n",
      "\"Revolution Enthusiast. History Fan.\" \n",
      "\n",
      "This introduction highlights your passion for the industrial revolution and its impact on history while keeping within the word limit.\n",
      "\n",
      "2: \"Revolution fan.\"\n",
      "\n",
      "3: \"IR enthusiast.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I like learning about the industrial revolution and how it shapes the modern world. How I can introduce myself in five words or less.\"\n",
    "\n",
    "# Generate the response multiple times by specifying a low temperature value\n",
    "for idx in range(3):\n",
    "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                    temperature=1)\n",
    "\n",
    "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Available models for the Chat endpoint](https://docs.cohere.com/docs/models#command)\n",
    "- [Documentation on predictable outputs](https://docs.cohere.com/v2/docs/predictable-outputs)\n",
    "- [Documentation on advanced generation parameters](https://docs.cohere.com/docs/advanced-generation-hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured output generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the `response_format` parameter, you can get the model to generate the output as a JSON object. By generating JSON objects, you can structure and organize the model's responses in a way that can be used in downstream applications.\n",
    "\n",
    "The `response_format` parameter allows you to specify the schema the JSON object must follow. It takes the following parameters:\n",
    "- `message`: The user message\n",
    "- `response_format`: The schema of the JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `response_format.schema` parameter is an experimental feature and may change in future releases.\n",
      "To suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Unable to Access Server', 'category': 'access', 'status': 'open'}\n"
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "user_input = \"Why can't I access the server? Is it a permissions issue?\"\n",
    "message = f\"\"\"Create an IT ticket for the following user request. Generate a JSON object.\n",
    "{user_input}\"\"\"\n",
    "\n",
    "# Generate the response multiple times by adding the JSON schema\n",
    "response = co.chat(\n",
    "  model=\"command-r-plus-08-2024\",\n",
    "  messages=[{\"role\": \"user\", \"content\": message}],\n",
    "  response_format={\n",
    "    \"type\": \"json_object\",\n",
    "    \"schema\": {\n",
    "      \"type\": \"object\",\n",
    "      \"required\": [\"title\", \"category\", \"status\"],\n",
    "      \"properties\": {\n",
    "        \"title\": { \"type\": \"string\"},\n",
    "        \"category\": { \"type\" : \"string\", \"enum\" : [\"access\", \"software\"]},\n",
    "        \"status\": { \"type\" : \"string\" , \"enum\" : [\"open\", \"closed\"]}\n",
    "      }\n",
    "    }\n",
    "  },\n",
    ")\n",
    "\n",
    "json_object = json.loads(response.message.content[0].text)\n",
    "\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Documentation on Structured Generations (JSON)](https://docs.cohere.com/docs/structured-outputs-json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the previous examples above generate responses in a non-streamed manner. This means that the endpoint would return a response object only after the model has generated the text in full.\n",
    "\n",
    "The Chat endpoint also provides streaming support. In a streamed response, the endpoint would return a response object for each token as it is being generated. This means you can display the text incrementally without having to wait for the full completion.\n",
    "\n",
    "To activate it, use `co.chat_stream()` instead of `co.chat()`.\n",
    "\n",
    "In streaming mode, the endpoint will generate a series of objects. To get the actual text contents, we take objects whose `event_type` is `content-delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hi, I'm [Your Name] and I'm thrilled to join the Co1t team today as a [Your Role], passionate about [Your Expertise], and excited to contribute to our shared mission of [Startup's Mission]!\""
     ]
    }
   ],
   "source": [
    "# Add the user message\n",
    "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
    "\n",
    "# Generate the response by streaming it\n",
    "response = co.chat_stream(model=\"command-r-plus-08-2024\",\n",
    "                          messages=[{\"role\": \"user\", \"content\": message}])\n",
    "\n",
    "for event in response:\n",
    "    if event:\n",
    "        if event.type == \"content-delta\":\n",
    "            print(event.delta.message.content.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "- [Documentation on streaming responses](https://docs.cohere.com/docs/streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you learned about:\n",
    "- How to get started with a basic text generation\n",
    "- How to improve outputs with prompt engineering\n",
    "- How to control outputs using parameter changes\n",
    "- How to generate structured outputs\n",
    "- How to stream text generation outputs\n",
    "\n",
    "However, we have only done all this using direct text generations. As its name implies, the Chat endpoint can also support building chatbots, which require features to support multi-turn conversations and maintain the conversation state. \n",
    "\n",
    "In Part 3, you'll learn how to build chatbots with the Chat endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
