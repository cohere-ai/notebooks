{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/notebooks/blob/main/notebooks/guides/agentic-rag/agentic_rag_pt1_routing.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing Queries to Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a RAG system that can search over diverse sources, such as a website, a database, and a set of documents.\n",
    "\n",
    "In a standard RAG setting, the application would aggregate retrieved documents from all the different sources it is connected to. This may contribute to noise from less relevant documents.\n",
    "\n",
    "Additionally, it doesn’t take into consideration that, given a data source's nature, it might be less or more relevant to a query than the other data sources.\n",
    "\n",
    "An agentic RAG system can solve this problem by routing queries to the most relevant tools based on the query's nature. This is done by leveraging the tool use capabilities of the Chat endpoint.\n",
    "\n",
    "In this tutorial, we'll cover:\n",
    "- Setting up the tools\n",
    "- Running an agentic RAG workflow\n",
    "- Routing queries to tools\n",
    "\n",
    "We'll build an agent that can answer questions about using Cohere, equipped with a number of different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, first we need to install the `cohere` library and create a Cohere client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cohere\n",
    "\n",
    "from tool_def import (\n",
    "    search_developer_docs,\n",
    "    search_developer_docs_tool,\n",
    "    search_internet,\n",
    "    search_internet_tool,\n",
    "    search_code_examples,\n",
    "    search_code_examples_tool,\n",
    ")\n",
    "\n",
    "co = cohere.ClientV2(api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an agentic RAG system, each data source is represented as a tool. A tool is broadly any function or service that can receive and send objects to the LLM. But in the case of RAG, this becomes a more specific case of a tool that takes a query as input and returns a set of documents.\n",
    "\n",
    "Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.\n",
    "- `search_developer_docs`: Searches Cohere developer documentation. Here we are creating a small list of sample documents for simplicity and will return the same list for every query. In practice, you will want to implement a search function such as those that use semantic search.\n",
    "- `search_internet`: Performs an internet search using Tavily search, which we take from LangChain's ready implementation.\n",
    "- `search_code_examples`: Searches for Cohere code examples and tutorials. Here we are also creating a small list of sample documents for simplicity.\n",
    "\n",
    "These functions are mapped to a dictionary called `functions_map` for easy access.\n",
    "\n",
    "Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects.\n",
    "\n",
    "Further reading:\n",
    "- [Documentation on parameter types in tool use](https://docs.cohere.com/v2/docs/parameter-types-in-tool-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_internet\": search_internet,\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and final setup step is to define the tool schemas in a format that can be passed to the Chat endpoint. A tool schema must contain the following fields: `name`, `description`, and `parameters` in the format shown below. \n",
    "\n",
    "This schema informs the LLM about what the tool does, which enables an LLM to decide whether to use a particular tool. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an agentic RAG workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run an agentic RAG workflow using a tool use approach. We can think of the system as consisting of four components:\n",
    "- The user\n",
    "- The application\n",
    "- The LLM\n",
    "- The tools\n",
    "\n",
    "At its most basic, these four components interact in a workflow through four steps:\n",
    "- **Step 1: Get user message** – The LLM gets the user message (via the application)\n",
    "- **Step 2: Tool planning and calling** – The LLM makes a decision on the tools to call (if any) and generates - the tool calls\n",
    "- **Step 3: Tool execution** - The application executes the tools and the sends the results to the LLM\n",
    "- **Step 4: Response and citation generation** – The LLM generates the response and citations to back to the user\n",
    "\n",
    "We wrap all these steps in a function called `run_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    search_developer_docs_tool,\n",
    "    search_internet_tool,\n",
    "    search_code_examples_tool\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"\"\"## Task and Context\n",
    "You are an assistant who helps developers use Cohere. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-r-plus-08-2024\"\n",
    "\n",
    "def run_agent(query, messages=None):\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "        \n",
    "    if \"system\" not in {m.get(\"role\") for m in messages}:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    # Step 1: get user message\n",
    "    print(f\"QUESTION:\\n{query}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    while response.message.tool_calls:\n",
    "        \n",
    "        print(\"TOOL PLAN:\")\n",
    "        print(response.message.tool_plan,\"\\n\")\n",
    "        print(\"TOOL CALLS:\")\n",
    "        for tc in response.message.tool_calls:\n",
    "            print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"tool_calls\": response.message.tool_calls, \"tool_plan\": response.message.tool_plan})        \n",
    "        \n",
    "        # Step 3: Get tool results\n",
    "        for tc in response.message.tool_calls:\n",
    "            tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "            tool_content = []\n",
    "            for data in tool_result:\n",
    "                tool_content.append({\"type\": \"document\", \"document\": {\"data\": json.dumps(data)}})\n",
    "                # Optional: add an \"id\" field in the \"document\" object, otherwise IDs are auto-generated\n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content})\n",
    "        \n",
    "        # Step 4: Generate response and citations \n",
    "        response = co.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            temperature=0.1\n",
    "        )\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
    "        \n",
    "    # Print final response\n",
    "    print(\"RESPONSE:\")\n",
    "    print(response.message.content[0].text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print citations (if any)\n",
    "    verbose_source = False # Change to True to display the contents of a source\n",
    "    if response.message.citations:\n",
    "        print(\"CITATIONS:\\n\")\n",
    "        for citation in response.message.citations:\n",
    "            print(f\"Start: {citation.start}| End:{citation.end}| Text:'{citation.text}' \")\n",
    "            print(\"Sources:\")\n",
    "            for idx, source in enumerate(citation.sources):\n",
    "                print(f\"{idx+1}. {source.id}\")\n",
    "                if verbose_source:\n",
    "                    print(f\"{source.tool_output}\")\n",
    "            print(\"\\n\") \n",
    "                    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing queries to tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the agent a few questions, starting with this one about the Embed endpoint.\n",
    "\n",
    "Because of question asks about a specific feature, the agent decides to use the `search_developer_docs` tool (instead of retrieving from all the data sources it's connected to).\n",
    "\n",
    "It first generates a tool plan that describes how it will handle the query. Then, it generates tool calls to the `search_developer_docs` tool with the associated `query` parameter.\n",
    "\n",
    "The tool does indeed contain the information asked by the user, which the agent then uses to generate its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "How many languages does Embed support?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'How many languages does Embed support?' \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"How many languages does Embed support?\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "The Embed endpoint supports over 100 languages.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 28| End:47| Text:'over 100 languages.' \n",
      "Sources:\n",
      "1. search_developer_docs_1s5qxhyswydy:2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"How many languages does Embed support?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now ask the agent a question about the authors of the sentence BERT paper. This information is not likely to be found in the developer documentation or code examples because it is not Cohere-specific, so we can expect the agent to use the internet search tool.\n",
    "\n",
    "And this is exactly what the agent does. This time, it decides to use the `search_internet` tool, triggers the search through Tavily search, and uses the results to generate its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Who are the authors of the sentence BERT paper?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for the authors of the sentence BERT paper. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_internet | Parameters: {\"query\":\"authors of the sentence BERT paper\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "Nils Reimers and Iryna Gurevych are the authors of the sentence BERT paper.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 0| End:4| Text:'Nils' \n",
      "Sources:\n",
      "1. search_internet_5am6cjesgdry:1\n",
      "\n",
      "\n",
      "Start: 5| End:12| Text:'Reimers' \n",
      "Sources:\n",
      "1. search_internet_5am6cjesgdry:0\n",
      "2. search_internet_5am6cjesgdry:1\n",
      "3. search_internet_5am6cjesgdry:3\n",
      "\n",
      "\n",
      "Start: 17| End:22| Text:'Iryna' \n",
      "Sources:\n",
      "1. search_internet_5am6cjesgdry:1\n",
      "\n",
      "\n",
      "Start: 23| End:31| Text:'Gurevych' \n",
      "Sources:\n",
      "1. search_internet_5am6cjesgdry:0\n",
      "2. search_internet_5am6cjesgdry:1\n",
      "3. search_internet_5am6cjesgdry:3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Who are the authors of the sentence BERT paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the agent a final question, this time about tutorials that are relevant for enterprises.\n",
    "\n",
    "Again, the agent uses the context of the query to decide on the most relevant tool. In this case, it selects the `search_code_examples` tool and provides a response based on the information found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      "Any tutorials that are relevant for enterprises?\n",
      "==================================================\n",
      "TOOL PLAN:\n",
      "I will search for 'tutorials for enterprises'. \n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_code_examples | Parameters: {\"query\":\"tutorials for enterprises\"}\n",
      "==================================================\n",
      "RESPONSE:\n",
      "I found one tutorial that is relevant for enterprises: Advanced Document Parsing For Enterprises.\n",
      "==================================================\n",
      "CITATIONS:\n",
      "\n",
      "Start: 55| End:97| Text:'Advanced Document Parsing For Enterprises.' \n",
      "Sources:\n",
      "1. search_code_examples_zkx3c2z7gzrs:4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Any tutorials that are relevant for enterprises?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we learned about:\n",
    "- How to set up tools in an agentic RAG system\n",
    "- How to run an agentic RAG workflow\n",
    "- How to automatically route queries to the most relevant data sources\n",
    "\n",
    "However, so far we have only seen rather simple queries. In practice, we may run into a complex query that needs to simplified, optimized, or split (etc.) before we can perform the retrieval.\n",
    "\n",
    "In Part 2, we'll learn how to build an agentic RAG system that can expand user queries into parallel queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
