{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5-gWUEqG4-T"
      },
      "source": [
        "# Creating a QA bot from technical documentation\n",
        "\n",
        "This notebook demonstrates how to create a chatbot (single turn) that answers user questions based on technical documentation made available to the model.\n",
        "\n",
        "We use the `aws-documentation` dataset ([link](https://github.com/siagholami/aws-documentation/tree/main)) for representativeness. This dataset contains 26k+ AWS documentation pages, preprocessed into 120k+ chunks, and 100 questions based on real user questions.\n",
        "\n",
        "We proceed as follows:\n",
        "1. Embed the AWS documentation into a vector database using Cohere's `embed` model and `llama_index`\n",
        "2. Build a retriever using Cohere's `rerank` for better accuracy, lower inference costs and lower latency\n",
        "3. Create model answers for the eval set of 100 questions using Cohere's `command-r` model\n",
        "4. Evaluate the generated answers against the golden answers of the eval set using `command-r+`, Cohere's most capable generative model as a judge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY2wHt3AG7X0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wajdtJmNG0Uy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install cohere datasets llama_index llama-index-llms-cohere llama-index-embeddings-cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kJ_9dyJxG0zA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/neel/Library/Caches/pypoetry/virtualenvs/notebooks-WcB_wkWy-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import cohere\n",
        "import datasets\n",
        "from llama_index.core import StorageContext, VectorStoreIndex, load_index_from_storage\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.embeddings.cohere import CohereEmbedding\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EECuI7OdIy8M"
      },
      "outputs": [],
      "source": [
        "# Set up Cohere client\n",
        "# TODO: delete before push\n",
        "api_key = os.getenv(\"COHERE_API_KEY\") # <your API key>\n",
        "co = cohere.Client(api_key=api_key, log_warning_experimental_features=False)\n",
        "\n",
        "stub_len = len(\"https://github.com/siagholami/aws-documentation/tree/main/documents/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UfRVoBIHmD"
      },
      "source": [
        "## 1. Embed technical documentation and store as vector database\n",
        "\n",
        "* Load the dataset from HuggingFace\n",
        "* Compute embeddings using Cohere's implementation in LlamaIndex, `CohereEmbedding`\n",
        "* Store inside a vector database, `VectorStoreIndex` from LlamaIndex\n",
        "\n",
        "\n",
        "Because this process is lengthy (~2h for all documents on a MacBookPro), we store the index to disc for future reuse. We also provide an option to to index only a subset of the data. This can be enabled by setting the value of the variable `USE_SNIPPET` to `True` in the block below. If you use this option, bear in mind that many documents will become unavailable to the model and, as a result, performance will suffer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoXezqvoG02f",
        "outputId": "cabfccf3-3c15-4955-dab9-e2734fa65a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'text', 'source', 'short_source'],\n",
            "    num_rows: 187147\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Set to true if you want to use only a small sample of the training data (~10000) rows\n",
        "USE_SNIPPET = True\n",
        "\n",
        "data = datasets.load_dataset(\"sauravjoshi23/aws-documentation-chunked\")\n",
        "# The data comes prechunked. We keep the data as-is in this notebook.\n",
        "# For more information on optimal preprocessing strategies, please check\n",
        "# our other notebooks!\n",
        "\n",
        "# Add a column with the shortened source. This will be used to cross-reference with the QA Evaluation dataset to ensure\n",
        "# that the relevant testing data sources are captured in the smaller samples of data selected\n",
        "if USE_SNIPPET:\n",
        "    data = data.map(lambda x: {\"short_source\": x[\"source\"][stub_len:].replace(\"/doc_source\", \"\")})\n",
        "\n",
        "# Build a mapping from sample id to index inside data (will be useful for retrieval later)\n",
        "map_id2index = {sample[\"id\"]: index for index, sample in enumerate(data[\"train\"])}\n",
        "\n",
        "print(data[\"train\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To assess the performance of our RAG pipeline, we will also need some QA pairs to validate the answers generated by `command`. The author of the repository above provides 100 QA pairs that we can test the model on. Let's download these questions.\n",
        "\n",
        "**NOTE**: if you have set `USE_SNIPPET` to `True`, the following block also creates a separate copy of the dataset with documents that the QA pairs test on, so that we can ensure that these particular documents are always included as a subset in the smaller sampled training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- to fix SSL error -- \n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# Load data from github\n",
        "url = \"https://github.com/siagholami/aws-documentation/blob/main/QA_true.csv?raw=true\"\n",
        "qa_pairs = pd.read_csv(url)\n",
        "\n",
        "# Filters and extracts the rows in the data that correspond with documents that are referenced\n",
        "# in the QA pairs test set\n",
        "if USE_SNIPPET:\n",
        "    golden_docs = qa_pairs['Document_True'].tolist()\n",
        "    golden_doc_data = data.filter(lambda x: x['short_source'] in golden_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tvgtKDBTG05h"
      },
      "outputs": [],
      "source": [
        "# Create index in vector database, and persist it for later reuse\n",
        "# Note: this cell takes about ~2h on a MacBookPro\n",
        "\n",
        "overwrite = False # only compute index if it doesn't exist\n",
        "path_index = Path(\".\") / \"aws-documentation_index_cohere\"\n",
        "\n",
        "# Select Cohere's new `embed-english-v3.0` as the engine to compute embeddings\n",
        "embed_model = CohereEmbedding(\n",
        "    cohere_api_key=api_key,\n",
        "    model_name=\"embed-english-v3.0\",\n",
        ")\n",
        " \n",
        "if not path_index.exists() or overwrite:\n",
        "    # Documents are prechunked. Keep them as-is for now\n",
        "    # -- for indexing on a subset of full dataset --\n",
        "    if USE_SNIPPET:\n",
        "        documents = [\n",
        "            TextNode(\n",
        "                text=data[\"train\"][index][\"text\"],\n",
        "                title=data[\"train\"][index][\"source\"][stub_len:],\n",
        "                id_=data[\"train\"][index][\"id\"],\n",
        "            ) for index in range(5_000)\n",
        "        ]\n",
        "        # Extend the sample of documents with the documents referenced in the QA pairs\n",
        "        # test set\n",
        "        documents.extend(\n",
        "            [\n",
        "                TextNode(\n",
        "                    text=data[\"text\"],\n",
        "                    title=data[\"source\"][stub_len:],\n",
        "                    id_=data[\"id\"],\n",
        "                ) for data in golden_doc_data[\"train\"]\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        documents = [\n",
        "            # -- for indexing full dataset --\n",
        "            TextNode(\n",
        "                text=sample[\"text\"],\n",
        "                title=sample[\"source\"][stub_len:], # save source minus stub\n",
        "                id_=sample[\"id\"],\n",
        "            ) for sample in data[\"train\"]\n",
        "        ]\n",
        "    \n",
        "    # Shuffle nodes in documents\n",
        "    random.shuffle(documents)\n",
        "\n",
        "    index = VectorStoreIndex(documents, embed_model=embed_model)\n",
        "    index.storage_context.persist(path_index)\n",
        "else:\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=path_index)\n",
        "    index = load_index_from_storage(storage_context, embed_model=embed_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS2yzRwOKHTv"
      },
      "source": [
        "## 2. Build a retriever using Cohere's `rerank`\n",
        "\n",
        "The vector database we built using `VectorStoreIndex` comes with an in-built retriever. We can call that retriever to fetch the top $k$ documents most relevant to the user question with:\n",
        "\n",
        "```python\n",
        "retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "```\n",
        "\n",
        "We recently released [Rerank-3](https://txt.cohere.com/rerank-3/) (April '24), which we can use to improve the quality of retrieval, as well as reduce latency and the cost of inference. To use the retriever with `rerank`, we create a thin wrapper around `index.as_retriever` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wy_BGGbGG08w"
      },
      "outputs": [],
      "source": [
        "class RetrieverWithRerank:\n",
        "    def __init__(self, retriever, api_key):\n",
        "        self.retriever = retriever\n",
        "        self.co = cohere.Client(api_key=api_key)\n",
        "\n",
        "    def retrieve(self, query: str, top_n: int):\n",
        "        # First call to the retriever fetches the closest indices\n",
        "        nodes = self.retriever.retrieve(query)\n",
        "        nodes = [\n",
        "            {\n",
        "                \"text\": node.node.text,\n",
        "                \"llamaindex_id\": node.node.id_,\n",
        "            }\n",
        "            for node\n",
        "            in nodes\n",
        "        ]\n",
        "        # Call co.rerank to improve the relevance of retrieved documents\n",
        "        reranked = self.co.rerank(query=query, documents=nodes, model=\"rerank-english-v3.0\", top_n=top_n)\n",
        "        nodes = [nodes[node.index] for node in reranked.results]\n",
        "        return nodes\n",
        "\n",
        "\n",
        "top_k = 60 # how many documents to fetch on first pass\n",
        "top_n = 20 # how many documents to sub-select with rerank\n",
        "\n",
        "# Instantiate retriver\n",
        "retriever = RetrieverWithRerank(\n",
        "    index.as_retriever(similarity_top_k=top_k),\n",
        "    api_key=api_key,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ODihI1YCG0_5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting an Auto Scaling group will stop and terminate all of your Amazon EC2 instances within that group. \n",
            "\n",
            "However, you may want to instead stop or terminate your Amazon EC2 instances individually, especially if you no longer need them running. This can be done using alarm actions in CloudWatch.\n",
            "\n",
            "If you want to remove an Auto Scaling group but keep your instances, you can follow the removal process described in the Amazon EC2 User Guide.\n"
          ]
        }
      ],
      "source": [
        "# Test the retriever on a single question!\n",
        "query = \"What happens to my Amazon EC2 instances if I delete my Auto Scaling group?\"\n",
        "\n",
        "# Retrieving relevant documents with rerank now fits in one line\n",
        "documents = retriever.retrieve(query, top_n=top_n)\n",
        "\n",
        "# Call Cohere's RAG pipeline with co.chat and the `documents` argument\n",
        "resp = co.chat(message=query, model=\"command-r\", temperature=0., documents=documents)\n",
        "print(resp.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHTKtvMTMLhz"
      },
      "source": [
        "This works! With `co.chat`, you get the additional benefit that citations are returned for every span of text. Here's a simple function to display the citations inside square brackets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7fH5mv2PG1DI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting an Auto Scaling group will stop and terminate [14]  all of your Amazon EC2 instances [14]  within that group. [14]  \n",
            "\n",
            "However, you may want to instead stop or terminate your Amazon EC2 instances individually [6] , especially if you no longer need them running. [6]  This can be done using alarm actions in CloudWatch. [6] \n",
            "\n",
            "If you want to remove an Auto Scaling group but keep your instances [18] , you can follow the removal process described in the Amazon EC2 User Guide. [18] \n"
          ]
        }
      ],
      "source": [
        "def build_answer_with_citations(response):\n",
        "    \"\"\" \"\"\"\n",
        "    text = response.text\n",
        "    citations = response.citations\n",
        "\n",
        "    # Construct text_with_citations adding citation spans as we iterate through citations\n",
        "    end = 0\n",
        "    text_with_citations = \"\"\n",
        "\n",
        "    for citation in citations:\n",
        "        # Add snippet between last citatiton and current citation\n",
        "        start = citation.start\n",
        "        text_with_citations += text[end : start]\n",
        "        end = citation.end  # overwrite\n",
        "        citation_blocks = \" [\" + \", \".join([stub[4:] for stub in citation.document_ids]) + \"] \"\n",
        "        text_with_citations += text[start : end] + citation_blocks\n",
        "    # Add any left-over\n",
        "    text_with_citations += text[end:]\n",
        "\n",
        "    return text_with_citations\n",
        "\n",
        "grounded_answer = build_answer_with_citations(resp)\n",
        "print(grounded_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1mJdBQYRI4k"
      },
      "source": [
        "## 3. Create model answers for 100 QA pairs\n",
        "\n",
        "Now that we have a running pipeline, we need to assess its performance.\n",
        "\n",
        "We can now run inference on all of the QA test pairs (100 questions). Later, we will use Command-R+ -- Cohere's largest and most powerful model -- to measure performance.\n",
        "\n",
        "We'll use the fields as follows:\n",
        "* `Question`: the user question, passed to `co.chat` to generate the answer\n",
        "* `Answer_True`: treat as the ground gruth; compare to the model-generated answer to determine its correctness\n",
        "* `Document_True`: treat as the (single) golden document; check the rank of this document inside the model's retrieved documents\n",
        "\n",
        "We'll loop over each question and generate our model answer. We'll also complete two steps that will be useful for evaluating our model next:\n",
        "1. We compute the rank of the golden document amid the retrieved documents -- this will inform how well our retrieval system performs\n",
        "2. We prepare the grading prompts -- these will be sent to an LLM scorer to compute the goodness of responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rR67DcP5epAV"
      },
      "outputs": [],
      "source": [
        "# Define the LLM eval prompt\n",
        "# We request a score and a reason for assigning that score to 'trigger CoT' and\n",
        "# improve the model response\n",
        "\n",
        "LLM_EVAL_TEMPLATE = \\\n",
        "\"\"\"\n",
        "## References\n",
        "{references}\n",
        "\n",
        "QUESTION: based on the above reference documents, answer the following question: {question}\n",
        "ANSWER: {answer}\n",
        "STUDENT RESPONSE: {completion}\n",
        "\n",
        "Based on the question (QUESTION) and answer above (ANSWER), grade the student's reponse (STUDENT RESPONSE). A correct response will contain exactly \\\n",
        "the same information as in the answer, even if it is worded differently. If the student's reponse is correct, \\\n",
        "give it a score of 1. Otherwise, give it a score of 0. Let's think step by step. Return your answer as \\\n",
        "as a compilable JSON with the following structure:\n",
        "{{\n",
        "    \"reasoning\": <reasoning>,\n",
        "    \"score: <score of 0 or 1>,\n",
        "}}\n",
        "\"\"\"\n",
        "# Response format to enforce the model to produce a JSONic structure with its response\n",
        "# as instructed by the eval prompt above\n",
        "RESPONSE_FORMAT = {\n",
        "    \"type\": \"json_object\",\n",
        "    \"schema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"required\": [\"reasoning\", \"score\"],\n",
        "        \"properties\": {\n",
        "            \"reasoning\": { \"type\": \"string\" },\n",
        "            \"score\": { \"type\": \"integer\"}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_rank_of_golden_within_retrieved(golden: str, retrieved: List[dict]) -> int:\n",
        "    \"\"\"\n",
        "    Returns the rank that the golden document (single) has within the retrieved documents\n",
        "    * `golden` contains the source of the document, e.g. 'amazon-ec2-user-guide/EBSEncryption.md'\n",
        "    * `retrieved` has a list of responses with key 'llamaindex_id', which links back to document sources\n",
        "    \"\"\"\n",
        "    # Create {document: rank} map using llamaindex_id (count first occurrence of any document; they can\n",
        "    # appear multiple times because they're chunked)\n",
        "    doc_to_rank = {}\n",
        "    for rank, doc in enumerate(retrieved):\n",
        "        # retrieve source of document\n",
        "        _id = doc[\"llamaindex_id\"]\n",
        "        source = data[\"train\"][map_id2index[_id]][\"source\"]\n",
        "        # format as in dataset\n",
        "        source = source[stub_len:]  # remove stub\n",
        "        source = source.replace(\"/doc_source\", \"\")  # remove /doc_source/\n",
        "        \n",
        "        if source not in doc_to_rank:\n",
        "            doc_to_rank[source] = rank + 1\n",
        "\n",
        "    # Return rank of `golden`, defaulting to len(retrieved) + 1 if it's absent\n",
        "    return doc_to_rank.get(golden, len(retrieved) + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tAg3MTOcMll4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [09:04<00:00,  5.44s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "answers = []\n",
        "golden_answers = []\n",
        "ranks = []\n",
        "grading_prompts = []  # best computed in batch\n",
        "\n",
        "for _, row in tqdm(qa_pairs.iterrows(), total=len(qa_pairs)):\n",
        "    query, golden_answer, golden_doc = row[\"Question\"], row[\"Answer_True\"], row[\"Document_True\"]\n",
        "    golden_answers.append(golden_answer)\n",
        "\n",
        "    # --- Produce answer using retriever ---\n",
        "    documents = retriever.retrieve(query, top_n=top_n)\n",
        "    resp = co.chat(\n",
        "        message=query,\n",
        "        model=\"command-r\",\n",
        "        temperature=0,\n",
        "        documents=documents,\n",
        "    )\n",
        "    answer = resp.text\n",
        "    answers.append(answer)\n",
        "\n",
        "    # --- Do some prework for evaluation later ---\n",
        "    # Rank\n",
        "    rank = get_rank_of_golden_within_retrieved(golden_doc, documents)\n",
        "    ranks.append(rank)\n",
        "    # Score: construct the grading prompts for LLM evals, then evaluate in batch\n",
        "    # Need to reformat documents slightly\n",
        "    documents = [{\"index\": str(i), \"text\": doc[\"text\"]} for i, doc in enumerate(documents)]\n",
        "    references_text = \"\\n\\n\".join(\"\\n\".join([f\"{k}: {v}\" for k, v in doc.items()]) for doc in documents)\n",
        "    # ^ snippet looks complicated, but all it does it unpack all kwargs from `documents`\n",
        "    # into text separated by \\n\\n\n",
        "    grading_prompt = LLM_EVAL_TEMPLATE.format(\n",
        "        references=references_text, question=query, answer=golden_answer, completion=answer\n",
        "    )\n",
        "    grading_prompts.append(grading_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Optional, to persist the grading_prompts list as a pickle object\n",
        "with open(\"../data/grading_prompts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(grading_prompts, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BCs7ozyO1vL"
      },
      "source": [
        "## 4. Evaluate model performance\n",
        "\n",
        "We want to test our model performance on two dimensions:\n",
        "1. How good is the final answer? We'll compare our model answer to the golden answer using Command-R+ as a judge.\n",
        "2. How good is the retrieval? We'll use the rank of the golden document within the retrieved documents to this end.\n",
        "\n",
        "Note that this pipeline is for illustration only. To measure performance in practice, we would want to run more in-depths tests on a broader, representative dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1qBvMpYmQDTK"
      },
      "outputs": [],
      "source": [
        "# For simplicity, prepare a DataFrame with the results\n",
        "results = pd.DataFrame()\n",
        "results[\"answer\"] = answers\n",
        "results[\"golden_answer\"] = qa_pairs[\"Answer_True\"]\n",
        "results[\"rank\"] = ranks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wglx0dsQtgX"
      },
      "source": [
        "### 4.1 Compare answer to golden answer\n",
        "\n",
        "We'll use Command-R+ as a judge of whether the answers produced by our model convey the same information as the golden answers. Since we've defined the grading prompts earlier, we can simply ask our LLM judge to evaluate that grading prompt. After a little bit of postprocessing, we can then extract our model scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_F3V4E56Q1CE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:43<00:00,  2.23s/it]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "scores = []\n",
        "reasonings = []\n",
        "\n",
        "def remove_backticks(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Some models are trained to output JSON in Markdown formatting:\n",
        "    ```json {json object}```\n",
        "    Remove the backticks from those model responses so that they become\n",
        "    parasable by json.loads.\n",
        "    \"\"\"\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[7:]\n",
        "    if text.endswith(\"```\"):\n",
        "        text = text[:-3]\n",
        "    return text\n",
        "\n",
        "\n",
        "# -- uncomment to load saved grading_prompts list --\n",
        "with open(\"../data/grading_prompts.pkl\", \"rb\") as f:\n",
        "    grading_prompts = pickle.load(f)\n",
        "\n",
        "for prompt in tqdm(grading_prompts, total=len(grading_prompts)):\n",
        "    resp = co.chat(\n",
        "       message=prompt, \n",
        "       model=\"command-r-plus\", \n",
        "       temperature=0., \n",
        "       response_format=RESPONSE_FORMAT,\n",
        "    )\n",
        "    # Convert response to JSON to extract the `score` and `reasoning` fields\n",
        "    # We remove backticks for compatibility with different LLMs\n",
        "    parsed = json.loads(resp.text)\n",
        "    scores.append(parsed[\"score\"])\n",
        "    reasonings.append(parsed[\"reasoning\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Optional, to persist scores and reasonings lists\n",
        "with open(\"../data/scores.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scores, f)\n",
        "with open(\"../data/reasonings.pkl\", \"wb\") as f:\n",
        "    pickle.dump(reasonings, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1xksywOeQ1IJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# -- uncomment to load saved scores, reasonings lists --\n",
        "with open(\"../data/scores.pkl\", \"rb\") as f:\n",
        "   scores = pickle.load(f)\n",
        "with open(\"../data/reasonings.pkl\", \"rb\") as f:\n",
        "   reasonings = pickle.load(f)\n",
        "\n",
        "# Add scores to our DataFrame\n",
        "results[\"score\"] = scores\n",
        "results[\"reasoning\"] = reasonings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.99"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[results[\"rank\"] <= 5].shape[0] / results.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l8Z2w1ERSeHZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average score: 0.970\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average score: {results['score'].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob-Km6dAPeHJ"
      },
      "source": [
        "### 4.2 Compute rank\n",
        "\n",
        "We've already computed the rank of the golden documents using `get_rank_of_golden_within_retrieved`. Here, we'll plot the histogram of ranks, using blue when the answer scored a 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SLn3s3n_MlpO"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\", rc={\"grid.color\": \".8\"})\n",
        "\n",
        "# Create a custom palette for score coloring\n",
        "custom_palette = {0: \"red\", 1: \"blue\"}\n",
        "\n",
        "f, ax = plt.subplots(figsize=(5, 3))\n",
        "sns.histplot(\n",
        "    data=results,\n",
        "    x=\"rank\",\n",
        "    hue=\"score\",\n",
        "    palette=custom_palette,\n",
        "    edgecolor=\"black\",\n",
        "    binwidth=1\n",
        ")\n",
        "\n",
        "ax.set_xticks([1, 5, 0, 10, 15, 20])\n",
        "ax.set_title(\"Rank of golden document (max means golden doc. wasn't retrieved)\")\n",
        "ax.set_xlabel(\"Rank\")\n",
        "\n",
        "legend = ax.get_legend()\n",
        "legend.set_title(\"Score\")\n",
        "legend.get_title().set_fontsize('small')\n",
        "legend.get_title().set_fontweight('bold')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZeTt7ijVKxo"
      },
      "source": [
        "We see that retrieval works well overall: for about over 95% of questions, the golden document is consistently within the top 5 documents. However, we also notice that approx. half the false answers come from instances where the golden document wasn't retrieved (`rank = top_k = 20`). This should be improved, e.g. by adding metadata to the documents such as their section headings, or altering the chunking strategy.\n",
        "\n",
        "There is also a non-negligible instance of false answers where the top document was retrieved. On closer inspection, many of these are due to the model phrasing its answers more verbosely than the (very laconic) golden documents. This highlights the importance of checking eval results before jumping to conclusions about model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q5i8EYDV_da"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "In this notebook, we've built a QA bot that answers user questions based on technical documentation. We've learnt:\n",
        "\n",
        "1. How to embed the technical documentation into a vector database using Cohere embeddings and `llama_index`\n",
        "2. How to build a custom retriever that leverages Cohere's `rerank`\n",
        "3. How to evaluate model performance against a predetermined set of golden QA pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC2FKrkSWcPn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
